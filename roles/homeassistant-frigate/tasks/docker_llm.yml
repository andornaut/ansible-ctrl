---
- name: Start LLM Docker containers
  vars:
    ollama_service_volumes:
      - "{{ homeassistantfrigate_ollama_required_volumes.data }}:/root/.ollama"
    ollama_volumes: "{{ common_volumes + ollama_service_volumes }}"

    openwebui_service_volumes:
      - "{{ homeassistantfrigate_openwebui_required_volumes.data }}:/app/backend/data"
    openwebui_volumes: "{{ common_volumes + openwebui_service_volumes }}"

  community.docker.docker_compose_v2:
    project_name: homeassistant-frigate-llm
    pull: always
    remove_orphans: true
    definition:
      networks: "{{ external_network }}"
      services:
        ollama:
          # http://ollama:11434
          container_name: ollama
          devices: "{{ homeassistantfrigate_ollama_devices }}"
          group_add:
            - "{{ getent_group.render[1] }}"
            - "{{ getent_group.video[1] }}"
          image: "{{ homeassistantfrigate_ollama_docker_image }}"
          logging: "{{ default_logging }}"
          restart: unless-stopped
          # https://docs.docker.com/reference/cli/docker/container/run/#security-opt
          # https://docs.docker.com/engine/security/seccomp/#run-without-the-default-seccomp-profile
          security_opt:
            - seccomp=unconfined
          volumes: "{{ ollama_volumes }}"

        openwebui:
          container_name: openwebui
          image: "{{ homeassistantfrigate_openwebui_docker_image }}"
          logging: "{{ default_logging }}"
          ports:
            - "{{ homeassistantfrigate_openwebui_port }}:8080"
          restart: unless-stopped
          volumes: "{{ openwebui_volumes }}"
